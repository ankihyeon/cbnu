{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc0938f56a44b5dbdddd6177256f13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4e79c9beb54c0ab62a5d286f331082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 파일 위치: C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-Deep-7.8B-AWQ\\snapshots\\2c461c2a15579f0e26ffad7e619f3a2ba2ce5e64\\config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# 모델 파일 중 하나의 경로를 가져옵니다 (예: config.json)\n",
    "model_file = hf_hub_download(repo_id=\"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\", filename=\"config.json\")\n",
    "\n",
    "print(\"모델 파일 위치:\", model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# 모델명 또는 로컬 저장 경로\n",
    "model_name = \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\"\n",
    "streaming = True    # choose the streaming option\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Choose your prompt:\n",
    "#   Math example (AIME 2024)\n",
    "prompt = r\"\"\"Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations:\n",
    "\\[\\log_2\\left({x \\over yz}\\right) = {1 \\over 2}\\]\\[\\log_2\\left({y \\over xz}\\right) = {1 \\over 3}\\]\\[\\log_2\\left({z \\over xy}\\right) = {1 \\over 4}\\]\n",
    "Then the value of $\\left|\\log_2(x^4y^3z^2)\\right|$ is $\\tfrac{m}{n}$ where $m$ and $n$ are relatively prime positive integers. Find $m+n$.\n",
    "\n",
    "Please reason step by step, and put your final answer within \\boxed{}.\"\"\"\n",
    "#   Korean MCQA example (CSAT Math 2025)\n",
    "prompt = r\"\"\"Question : $a_1 = 2$인 수열 $\\{a_n\\}$과 $b_1 = 2$인 등차수열 $\\{b_n\\}$이 모든 자연수 $n$에 대하여\\[\\sum_{k=1}^{n} \\frac{a_k}{b_{k+1}} = \\frac{1}{2} n^2\\]을 만족시킬 때, $\\sum_{k=1}^{5} a_k$의 값을 구하여라.\n",
    "\n",
    "Options :\n",
    "A) 120\n",
    "B) 125\n",
    "C) 130\n",
    "D) 135\n",
    "E) 140\n",
    " \n",
    "Please reason step by step, and you should write the correct option alphabet (A, B, C, D or E) within \\\\boxed{}.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "if streaming:\n",
    "    streamer = TextIteratorStreamer(tokenizer)\n",
    "    thread = Thread(target=model.generate, kwargs=dict(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=32768,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        streamer=streamer\n",
    "    ))\n",
    "    thread.start()\n",
    "\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "else:\n",
    "    output = model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=32768,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdce75ea5934dc795b75e8ad161d57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m streaming \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     12\u001b[0m     model_name,\n\u001b[0;32m     13\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     14\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[0;32m     15\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[39m# 토크나이저 로드\u001b[39;00m\n\u001b[0;32m     19\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    561\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\modeling_utils.py:3916\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3906\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3907\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3909\u001b[0m     (\n\u001b[0;32m   3910\u001b[0m         model,\n\u001b[0;32m   3911\u001b[0m         missing_keys,\n\u001b[0;32m   3912\u001b[0m         unexpected_keys,\n\u001b[0;32m   3913\u001b[0m         mismatched_keys,\n\u001b[0;32m   3914\u001b[0m         offload_index,\n\u001b[0;32m   3915\u001b[0m         error_msgs,\n\u001b[1;32m-> 3916\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   3917\u001b[0m         model,\n\u001b[0;32m   3918\u001b[0m         state_dict,\n\u001b[0;32m   3919\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3920\u001b[0m         resolved_archive_file,\n\u001b[0;32m   3921\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3922\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   3923\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   3924\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   3925\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   3926\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3927\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3928\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   3929\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   3930\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[0;32m   3931\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3932\u001b[0m         gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[0;32m   3933\u001b[0m     )\n\u001b[0;32m   3935\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   3936\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\modeling_utils.py:4390\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[0;32m   4386\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   4387\u001b[0m                     model_to_load, key, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mempty(\u001b[39m*\u001b[39mparam\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m   4388\u001b[0m                 )\n\u001b[0;32m   4389\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4390\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[0;32m   4391\u001b[0m             model_to_load,\n\u001b[0;32m   4392\u001b[0m             state_dict,\n\u001b[0;32m   4393\u001b[0m             loaded_keys,\n\u001b[0;32m   4394\u001b[0m             start_prefix,\n\u001b[0;32m   4395\u001b[0m             expected_keys,\n\u001b[0;32m   4396\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   4397\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   4398\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[0;32m   4399\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[0;32m   4400\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[0;32m   4401\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   4402\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[0;32m   4403\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[0;32m   4404\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   4405\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[0;32m   4406\u001b[0m         )\n\u001b[0;32m   4407\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[0;32m   4408\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4409\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\modeling_utils.py:936\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[0;32m    925\u001b[0m     state_dict_index \u001b[39m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[0;32m    926\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    927\u001b[0m     \u001b[39mnot\u001b[39;00m is_quantized\n\u001b[0;32m    928\u001b[0m     \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m hf_quantizer\u001b[39m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m ):\n\u001b[0;32m    935\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[1;32m--> 936\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mset_module_kwargs)\n\u001b[0;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\accelerate\\utils\\modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[0;32m    414\u001b[0m             module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m param_cls(new_value, requires_grad\u001b[39m=\u001b[39mold_value\u001b[39m.\u001b[39mrequires_grad)\n\u001b[0;32m    415\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 416\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m    417\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    418\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "from awq import AutoAWQForCausalLM  # 여기 핵심\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델명 또는 로컬 저장 경로\n",
    "model_name = \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\"\n",
    "streaming = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#  프롬프트 구성\n",
    "instruction = r\"\"\"\n",
    "Role(역할지정):  \n",
    "개인정보 식별 및 JSON 변환 전문가로 활동하세요.  \n",
    "\n",
    "Context(상황):  \n",
    "- 사용자가 입력한 문장에서 개인정보 항목을 식별하고, 이를 JSON 형식으로 변환해야 합니다.  \n",
    "- **문장에 포함되지 않은 정보는 null 값이 아닌 빈 배열 []로 출력해야 합니다.**  \n",
    "- **각 항목별로 여러 개의 데이터가 있을 수 있으며, 이를 리스트(Array) 형태로 저장해야 합니다.**  \n",
    "- **아래 명시된 JSON 구조 외의 다른 필드(예: \"other\")는 절대 포함하지 마세요.**  \n",
    "- **각 항목은 정확한 기준에 따라 매칭해야 하며, 오탐을 방지해야 합니다.**  \n",
    "\n",
    "1. 이름(name)  \n",
    "- **여러 개의 이름이 존재하는 경우 리스트(Array) 형태로 반환해야 합니다.**  \n",
    "  - **\"고객명\", \"성함\", \"이름\"** 등의 키워드가 포함된 값은 반드시 \"name\" 필드로 저장해야 합니다.  \n",
    "  - 쉼표(,), ‘및’, ‘와/과’ 같은 연결어로 구분된 경우 각각 개별 요소로 추출  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 직급(예: 대표, 부장, 차장, 과장, 사원 등)  \n",
    "  - 회사명(예: (주)위상, ABC Corp, Google Korea 등)  \n",
    "  - 존칭(예: 님, 씨, 선생님 등)  \n",
    "  - 서명 표현(예: 배상, 올림, 드림)  \n",
    "  - 쉼표(,) 이후 불필요한 정보(예: \"김철수, 마케팅팀\")  \n",
    "\n",
    "2. 주소(address)  \n",
    "- **여러 개의 주소가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 키워드가 포함된 문장을 주소로 인식:**  \n",
    "  - \"건물위치\", \"주소\", \"위치\", \"사업장 주소\", \"본사 주소\", \"지점 주소\"  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 전화번호와 붙어 있는 경우 전화번호 제거  \n",
    "  - 주소 형식이 아닌 일반적인 지역 언급 제거 (예: \"상담 가능 지역: 서울, 경기\")  \n",
    "\n",
    "3. 전화번호(phone_number)  \n",
    "- **여러 개의 전화번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 국내 휴대전화번호 (예: 010-1234-5678)  \n",
    "  - 국내 일반전화 (예: 02-1234-5678)  \n",
    "  - 국제전화 형식 (예: +82-10-1234-5678)  \n",
    "- **숫자 조합이지만 전화번호가 아닌 경우 제거** (예: \"사업자 등록번호 123-45-67890\")  \n",
    "\n",
    "4. 이메일(email)  \n",
    "- **여러 개의 이메일이 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **이메일 형식을 만족하는 경우만 추출**  \n",
    "\n",
    "5. 신분증 번호(id_number)  \n",
    "- **여러 개의 신분증 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 운전면허증 번호 (예: 12-34-567890-12)  \n",
    "  - 여권 번호 (예: M12345678)  \n",
    "\n",
    "6. 주민등록번호(resident_registration_number)  \n",
    "- **여러 개의 주민등록번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "7. 은행 계좌번호(bank_account)  \n",
    "- **여러 개의 은행 계좌번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "8. 운송장 번호(tracking_number)  \n",
    "- **여러 개의 운송장 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "---\n",
    "\n",
    "Input Values(입력값):  \n",
    "- 사용자가 입력한 문장  \n",
    "\n",
    "Instructions(단계별 지시사항):  \n",
    "1. 입력된 문장에서 개인정보 항목을 식별하세요.  \n",
    "2. **각 항목별로 여러 개의 값이 존재할 경우 리스트(Array) 형태로 저장하세요.**  \n",
    "3. **\"other\" 필드는 절대 생성하지 마세요.**  \n",
    "4. **\"name\" 필드는 \"고객명\", \"성함\", \"이름\" 키워드가 포함된 값을 저장해야 합니다.**  \n",
    "5. 해당 항목이 존재하면 값을 리스트로 반환하고, 존재하지 않으면 **빈 배열([])**을 할당하세요.  \n",
    "6. 모든 정보를 JSON 형식으로 변환하세요.  \n",
    "\n",
    "Constraints(제약사항):  \n",
    "- **불필요한 필드는 출력하지 않습니다.**  \n",
    "- **각 항목별 정제 기준을 엄격하게 적용하여 오탐을 방지합니다.**  \n",
    "- 출력은 반드시 JSON 형식으로 작성하세요.  \n",
    "- 한국어로 답변하세요.  \n",
    "\n",
    "---\n",
    "\n",
    "예제: 필드 정제 후 JSON 출력 (불필요한 \"other\" 필드 제거)**\n",
    "\"\"\"\n",
    "\n",
    "input_text = r\"\"\"\n",
    "[심의요청] STUDIO TOMBOY\n",
    "고객명 : 김민지\n",
    "휴대폰 : 010-1111-2222\n",
    "구입일 : 2024-05-17\n",
    "구매금액 : 85,960원\n",
    "구매처 :(당사 주문은 주문 번호) SIV / 주문번호 202405173238829\n",
    "브랜드 : STUDIO TOMBOY\n",
    "구매점 : SIV\n",
    "SKU : 9104222391085OS\n",
    "상품명 : [2403632642] YOUTH 피그먼트 피케 티셔츠\n",
    "요청내용 :\n",
    "ㅁ 세탁 후 물 빠짐 / 전체적으로 발생, 흰색 줄 있는 듯한 모양으로 빠짐\n",
    "\"\"\"\n",
    "\n",
    "# 채팅 메시지 구성\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction.strip()},\n",
    "    {\"role\": \"user\", \"content\": input_text.strip()}\n",
    "]\n",
    "\n",
    "# 토큰화\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 스트리밍 or 비스트리밍\n",
    "if streaming:\n",
    "    streamer = TextIteratorStreamer(tokenizer)\n",
    "    thread = Thread(target=model.generate, kwargs=dict(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        streamer=streamer\n",
    "    ))\n",
    "    thread.start()\n",
    "\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "else:\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\awq\\modules\\linear\\exllama.py:12: UserWarning: AutoAWQ could not load ExLlama kernels extension. Details: DLL load failed while importing exl_ext: 지정된 모듈을 찾을 수 없습니다.\n",
      "  warnings.warn(f\"AutoAWQ could not load ExLlama kernels extension. Details: {ex}\")\n",
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\awq\\modules\\linear\\exllamav2.py:13: UserWarning: AutoAWQ could not load ExLlamaV2 kernels extension. Details: DLL load failed while importing exlv2_ext: 지정된 모듈을 찾을 수 없습니다.\n",
      "  warnings.warn(f\"AutoAWQ could not load ExLlamaV2 kernels extension. Details: {ex}\")\n",
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\awq\\modules\\linear\\gemm.py:14: UserWarning: AutoAWQ could not load GEMM kernels extension. Details: DLL load failed while importing awq_ext: 지정된 모듈을 찾을 수 없습니다.\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMM kernels extension. Details: {ex}\")\n",
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\awq\\modules\\linear\\gemv.py:11: UserWarning: AutoAWQ could not load GEMV kernels extension. Details: DLL load failed while importing awq_ext: 지정된 모듈을 찾을 수 없습니다.\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMV kernels extension. Details: {ex}\")\n",
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\awq\\modules\\linear\\gemv_fast.py:10: UserWarning: AutoAWQ could not load GEMVFast kernels extension. Details: DLL load failed while importing awq_v2_ext: 지정된 모듈을 찾을 수 없습니다.\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMVFast kernels extension. Details: {ex}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b79fe5f6504eaba9589efadafe200f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_exaone.py:   0%|          | 0.00/9.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ:\n",
      "- configuration_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30478abaa1414636b503ea831bd53be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_exaone.py:   0%|          | 0.00/63.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ:\n",
      "- modeling_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9b4d1eb5994fa195da067a25acc374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/61.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304c00d84c5e4d6ab666902c8ff828ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da890d5c6957428d9c7703db4c85a4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff8ee095f7f4065a00e66189fde942a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/839M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce82925678f461fbad22fa8f5d6350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49e2dcf359348c58e2c115d920083f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/223 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c93cb44de33402e809a6c1c8b44d5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/70.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69440f5a25ec483ca8d3b7fab44957a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9191dfef32e48028927eb63c50ada1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a157754e24d24551b5b6009ad45086d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b1ce7666a442f98fffb8ca9cdb00cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\LGAI-EXAONE\\EXAONE-Deep-7.8B-AWQ\\2c461c2a15579f0e26ffad7e619f3a2ba2ce5e64\\modeling_exaone.py:529: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's tackle this problem step by step. The user provided a query where they need to extract specific personal information from a given text and format it into a JSON structure. The instructions are quite detailed, so I need to make sure I follow each guideline precisely.\n",
      "\n",
      "First, I need to identify all the required fields from the JSON structure provided in the example. The fields are name, address, phone_number, email, id_number, resident_registration_number, bank_account, and tracking_number. Each of these should be an array, even if empty.\n",
      "\n",
      "Looking at the input text provided:\n",
      "\n",
      "- **Name**: The text mentions \"김민지\" which is \"Kim Min-ji\" in Korean. Since the instruction says to include names with keywords like \"고객명\", \"성함\", \"이름\", this should be under the \"name\" field as an array. But since it's a single name, it will be a single-element array.\n",
      "\n",
      "- **Phone Number**: The phone number given is \"010-1111-2222\". The instructions specify that domestic mobile numbers (starting with 010 or 02) should be included. This fits, so it goes into \"phone_number\" array.\n",
      "\n",
      "- **Email**: There's no email address in the text, so the email array remains empty.\n",
      "\n",
      "- **ID Numbers**: The text has \"구입일\" (purchase date), \"구매금액\" (amount), \"주문번호\" (order number), \"브랜드\" (brand), \"구매점\" (place of purchase), and \"SKU\". However, the instructions specify that only certain ID numbers are to be extracted. The \"id_number\" field is for things like driving licenses, and \"resident_registration_number\" is for resident IDs. The order number and SKU are probably not considered here. The purchase date and amount are dates/numbers but not specific ID numbers. So these might not be included. Wait, but maybe \"주문번호\" is a reference number, but according to the constraints, only specific types like driving licenses or resident IDs are included. So perhaps these are excluded. So both \"id_number\" and \"resident_registration_number\" would be empty arrays.\n",
      "\n",
      "- **Address**: The text mentions \"구매처 : (당사 주문은 주문 번호) SIV / 주문번호 202405173238829\" which translates to \"Purchase place: (Company order is order number) SIV / Order number 202405173238829\". The address here might be the SIV part, but the instructions for address require keywords like \"건물위치\", \"주소\", etc. The text here uses \"구매처\" which is \"purchase place\", but that's not one of the specified keywords. The other part is \"SIV\", which might stand for a service, but not an address. So maybe there's no valid address here, so address array remains empty.\n",
      "\n",
      "- **Bank Account**: No information about bank accounts is provided, so this stays empty.\n",
      "\n",
      "- **Tracking Number**: Similarly, no tracking numbers are mentioned, so empty array.\n",
      "\n",
      "Wait, but let me double-check each field again to ensure I didn't miss anything.\n",
      "\n",
      "Looking again:\n",
      "\n",
      "- The \"브랜드\" (brand) is "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 177\u001b[0m\n\u001b[0;32m    174\u001b[0m thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    176\u001b[0m full_response \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 177\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m streamer:\n\u001b[0;32m    178\u001b[0m     \u001b[39mprint\u001b[39m(text, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    179\u001b[0m     full_response \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m text\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\generation\\streamers.py:223\u001b[0m, in \u001b[0;36mTextIteratorStreamer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 223\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_signal:\n\u001b[0;32m    225\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\queue.py:170\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[0;32m    171\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    172\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\LLM\\lib\\threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "from awq import AutoAWQForCausalLM  # 여기 핵심\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델명 또는 로컬 저장 경로\n",
    "model_name = \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\"\n",
    "streaming = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16  # 또는 float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#  프롬프트 구성\n",
    "instruction = r\"\"\"\n",
    "Role(역할지정):  \n",
    "개인정보 식별 및 JSON 변환 전문가로 활동하세요.\n",
    "\n",
    "Context(상황):  \n",
    "- 사용자가 입력한 문장에서 개인정보 항목을 식별하고, 이를 JSON 형식으로 변환해야 합니다.  \n",
    "- 문장에 포함되지 않은 정보는 null 값이 아닌 빈 배열 []로 출력해야 합니다.  \n",
    "- 각 항목별로 여러 개의 데이터가 있을 수 있으며, 이를 리스트(Array) 형태로 저장해야 합니다.  \n",
    "- 아래 명시된 JSON 구조 외의 다른 필드(예: \"other\")는 절대 포함하지 마세요.  \n",
    "- 각 항목은 정확한 기준에 따라 매칭해야 하며, 오탐을 방지해야 합니다.\n",
    "\n",
    "1. 이름(name)  \n",
    "- **여러 개의 이름이 존재하는 경우 리스트(Array) 형태로 반환해야 합니다.**  \n",
    "  - **\"고객명\", \"성함\", \"이름\"** 등의 키워드가 포함된 값은 반드시 \"name\" 필드로 저장해야 합니다.  \n",
    "  - 쉼표(,), ‘및’, ‘와/과’ 같은 연결어로 구분된 경우 각각 개별 요소로 추출  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 직급(예: 대표, 부장, 차장, 과장, 사원 등)  \n",
    "  - 회사명(예: (주)위상, ABC Corp, Google Korea 등)  \n",
    "  - 존칭(예: 님, 씨, 선생님 등)  \n",
    "  - 서명 표현(예: 배상, 올림, 드림)  \n",
    "  - 쉼표(,) 이후 불필요한 정보(예: \"김철수, 마케팅팀\")  \n",
    "\n",
    "2. 주소(address)  \n",
    "- **여러 개의 주소가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 키워드가 포함된 문장을 주소로 인식:**  \n",
    "  - \"건물위치\", \"주소\", \"위치\", \"사업장 주소\", \"본사 주소\", \"지점 주소\"  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 전화번호와 붙어 있는 경우 전화번호 제거  \n",
    "  - 주소 형식이 아닌 일반적인 지역 언급 제거 (예: \"상담 가능 지역: 서울, 경기\")  \n",
    "\n",
    "3. 전화번호(phone_number)  \n",
    "- **여러 개의 전화번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 국내 휴대전화번호 (예: 010-1234-5678)  \n",
    "  - 국내 일반전화 (예: 02-1234-5678)  \n",
    "  - 국제전화 형식 (예: +82-10-1234-5678)  \n",
    "- **숫자 조합이지만 전화번호가 아닌 경우 제거** (예: \"사업자 등록번호 123-45-67890\")  \n",
    "\n",
    "4. 이메일(email)  \n",
    "- **여러 개의 이메일이 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **이메일 형식을 만족하는 경우만 추출**  \n",
    "\n",
    "5. 신분증 번호(id_number)  \n",
    "- **여러 개의 신분증 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 운전면허증 번호 (예: 12-34-567890-12)  \n",
    "  - 여권 번호 (예: M12345678)  \n",
    "\n",
    "6. 주민등록번호(resident_registration_number)  \n",
    "- **여러 개의 주민등록번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "7. 은행 계좌번호(bank_account)  \n",
    "- **여러 개의 은행 계좌번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "8. 운송장 번호(tracking_number)  \n",
    "- **여러 개의 운송장 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "---\n",
    "\n",
    "Instructions(지시사항):  \n",
    "- 입력 문장에서 다음 8가지 항목을 식별하세요:  \n",
    "  1. name  \n",
    "  2. address  \n",
    "  3. phone_number  \n",
    "  4. email  \n",
    "  5. id_number  \n",
    "  6. resident_registration_number  \n",
    "  7. bank_account  \n",
    "  8. tracking_number  \n",
    "\n",
    "- 모든 항목은 반드시 리스트(Array) 형태로 출력해야 하며, 항목이 존재하지 않으면 빈 배열([])로 출력합니다.  \n",
    "- 절대로 \"other\" 필드나, 명시되지 않은 기타 필드는 포함하지 마세요.  \n",
    "- 분석, 해설, 주석 없이 **오직 JSON 코드 블록만 출력하세요.**\n",
    "- 출력은 반드시 다음과 같은 **순수 JSON 형식의 코드 블록**으로 반환되어야 합니다:  \n",
    "\n",
    "Constraints(제약사항):  \n",
    "- **불필요한 필드는 출력하지 않습니다.**  \n",
    "- **각 항목별 정제 기준을 엄격하게 적용하여 오탐을 방지합니다.**  \n",
    "- 각 항목은 반드시 리스트([]) 형태로 출력해야 합니다.  \n",
    "- 해당 항목이 없으면 빈 배열([])로 출력합니다.  \n",
    "- 불필요한 설명, 분석 과정 없이 **JSON만 출력**해야 합니다.  \n",
    "- \"other\" 같은 불필요한 필드는 절대 포함하지 마세요.  \n",
    "- 출력은 반드시 JSON 형식으로 작성하세요.  \n",
    "- 답변은 한국어가 아닌 JSON 코드만 반환하세요.\n",
    "\n",
    "최종 출력은 반드시 아래 형식의 JSON 코드 블록만 포함되어야 합니다.  \n",
    "그 외의 텍스트, 분석, 해석은 절대 출력하지 마세요.\n",
    "\n",
    "예시:\n",
    "```json\n",
    "{\n",
    "  \"name\": [],\n",
    "  \"address\": [],\n",
    "  \"phone_number\": [\"],\n",
    "  \"email\": [],\n",
    "  \"id_number\": [],\n",
    "  \"resident_registration_number\": [],\n",
    "  \"bank_account\": [],\n",
    "  \"tracking_number\": []\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_text = r\"\"\"\n",
    "[심의요청] STUDIO TOMBOY\n",
    "고객명 : 김민지\n",
    "휴대폰 : 010-1111-2222\n",
    "구입일 : 2024-05-17\n",
    "구매금액 : 85,960원\n",
    "구매처 :(당사 주문은 주문 번호) SIV / 주문번호 202405173238829\n",
    "브랜드 : STUDIO TOMBOY\n",
    "구매점 : SIV\n",
    "SKU : 9104222391085OS\n",
    "상품명 : [2403632642] YOUTH 피그먼트 피케 티셔츠\n",
    "요청내용 :\n",
    "ㅁ 세탁 후 물 빠짐 / 전체적으로 발생, 흰색 줄 있는 듯한 모양으로 빠짐\n",
    "\"\"\"\n",
    "\n",
    "# 채팅 메시지 구성\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction.strip()},\n",
    "    {\"role\": \"user\", \"content\": input_text.strip()}\n",
    "]\n",
    "\n",
    "# 토큰화\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 스트리밍 or 비스트리밍\n",
    "if streaming:\n",
    "    # 스트리머에 옵션 추가\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 생성 실행 스레드\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=2048,  # 넉넉하게 확보\n",
    "        do_sample=False,       # JSON 포맷 정확도 위해 비활성화 (선택)\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "\n",
    "    thread.join()  # 스트리밍이 끝날 때까지 기다림\n",
    "else:\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=2024,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    \n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ JSON 형식 검증 완료: 모든 필드가 정확히 구성되어 있습니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 예시 출력 문자열 (실제 LLM 모델 출력값을 여기에 붙여넣기)\n",
    "llm_output = \"\"\"\n",
    "{\n",
    "  \"name\": [\"김민지\"],\n",
    "  \"address\": [],\n",
    "  \"phone_number\": [\"010-1111-2222\"],\n",
    "  \"email\": [],\n",
    "  \"id_number\": [],\n",
    "  \"resident_registration_number\": [],\n",
    "  \"bank_account\": [],\n",
    "  \"tracking_number\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 결과 검증 함수\n",
    "def validate_personal_info_json(output: str):\n",
    "    try:\n",
    "        # JSON 파싱\n",
    "        data = json.loads(output)\n",
    "\n",
    "        # 필수 키 목록\n",
    "        required_keys = [\n",
    "            \"name\", \"address\", \"phone_number\", \"email\", \"id_number\",\n",
    "            \"resident_registration_number\", \"bank_account\", \"tracking_number\"\n",
    "        ]\n",
    "\n",
    "        # 모든 필드 존재 여부 확인 및 타입 검사\n",
    "        for key in required_keys:\n",
    "            if key not in data:\n",
    "                return f\"❌ 누락된 필드: {key}\"\n",
    "            if not isinstance(data[key], list):\n",
    "                return f\"❌ {key} 필드는 리스트(Array)여야 합니다.\"\n",
    "\n",
    "        # 불필요한 필드가 포함되어 있는지 확인\n",
    "        for key in data.keys():\n",
    "            if key not in required_keys:\n",
    "                return f\"❌ 허용되지 않은 필드가 포함됨: {key}\"\n",
    "\n",
    "        return \"✅ JSON 형식 검증 완료: 모든 필드가 정확히 구성되어 있습니다.\"\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        return \"❌ JSON 디코딩 실패: 출력이 올바른 JSON 형식이 아닙니다.\"\n",
    "\n",
    "# 검증 실행\n",
    "validate_personal_info_json(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\",\n",
    "    cache_dir=\"E:/LLM/exaone-7.8b-awq\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\",\n",
    "    cache_dir=\"E:/LLM/exaone-7.8b-awq\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"E:/LLM/exaone-7.8b-awq\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"E:/LLM/exaone-7.8b-awq\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "#  프롬프트 구성\n",
    "instruction = r\"\"\"\n",
    "Role(역할지정):  \n",
    "개인정보 식별 및 JSON 변환 전문가로 활동하세요.\n",
    "\n",
    "Context(상황):  \n",
    "- 사용자가 입력한 문장에서 개인정보 항목을 식별하고, 이를 JSON 형식으로 변환해야 합니다.  \n",
    "- 문장에 포함되지 않은 정보는 null 값이 아닌 빈 배열 []로 출력해야 합니다.  \n",
    "- 각 항목별로 여러 개의 데이터가 있을 수 있으며, 이를 리스트(Array) 형태로 저장해야 합니다.  \n",
    "- 아래 명시된 JSON 구조 외의 다른 필드(예: \"other\")는 절대 포함하지 마세요.  \n",
    "- 각 항목은 정확한 기준에 따라 매칭해야 하며, 오탐을 방지해야 합니다.\n",
    "\n",
    "1. 이름(name)  \n",
    "- **여러 개의 이름이 존재하는 경우 리스트(Array) 형태로 반환해야 합니다.**  \n",
    "  - **\"고객명\", \"성함\", \"이름\"** 등의 키워드가 포함된 값은 반드시 \"name\" 필드로 저장해야 합니다.  \n",
    "  - 쉼표(,), ‘및’, ‘와/과’ 같은 연결어로 구분된 경우 각각 개별 요소로 추출  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 직급(예: 대표, 부장, 차장, 과장, 사원 등)  \n",
    "  - 회사명(예: (주)위상, ABC Corp, Google Korea 등)  \n",
    "  - 존칭(예: 님, 씨, 선생님 등)  \n",
    "  - 서명 표현(예: 배상, 올림, 드림)  \n",
    "  - 쉼표(,) 이후 불필요한 정보(예: \"김철수, 마케팅팀\")  \n",
    "\n",
    "2. 주소(address)  \n",
    "- **여러 개의 주소가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 키워드가 포함된 문장을 주소로 인식:**  \n",
    "  - \"건물위치\", \"주소\", \"위치\", \"사업장 주소\", \"본사 주소\", \"지점 주소\"  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 전화번호와 붙어 있는 경우 전화번호 제거  \n",
    "  - 주소 형식이 아닌 일반적인 지역 언급 제거 (예: \"상담 가능 지역: 서울, 경기\")  \n",
    "\n",
    "3. 전화번호(phone_number)  \n",
    "- **여러 개의 전화번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 국내 휴대전화번호 (예: 010-1234-5678)  \n",
    "  - 국내 일반전화 (예: 02-1234-5678)  \n",
    "  - 국제전화 형식 (예: +82-10-1234-5678)  \n",
    "- **숫자 조합이지만 전화번호가 아닌 경우 제거** (예: \"사업자 등록번호 123-45-67890\")  \n",
    "\n",
    "4. 이메일(email)  \n",
    "- **여러 개의 이메일이 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **이메일 형식을 만족하는 경우만 추출**  \n",
    "\n",
    "5. 신분증 번호(id_number)  \n",
    "- **여러 개의 신분증 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 운전면허증 번호 (예: 12-34-567890-12)  \n",
    "  - 여권 번호 (예: M12345678)  \n",
    "\n",
    "6. 주민등록번호(resident_registration_number)  \n",
    "- **여러 개의 주민등록번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "7. 은행 계좌번호(bank_account)  \n",
    "- **여러 개의 은행 계좌번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "8. 운송장 번호(tracking_number)  \n",
    "- **여러 개의 운송장 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "---\n",
    "\n",
    "Instructions(지시사항):  \n",
    "- 입력 문장에서 다음 8가지 항목을 식별하세요:  \n",
    "  1. name  \n",
    "  2. address  \n",
    "  3. phone_number  \n",
    "  4. email  \n",
    "  5. id_number  \n",
    "  6. resident_registration_number  \n",
    "  7. bank_account  \n",
    "  8. tracking_number  \n",
    "\n",
    "- 모든 항목은 반드시 리스트(Array) 형태로 출력해야 하며, 항목이 존재하지 않으면 빈 배열([])로 출력합니다.  \n",
    "- 절대로 \"other\" 필드나, 명시되지 않은 기타 필드는 포함하지 마세요.  \n",
    "- 분석, 해설, 주석 없이 **오직 JSON 코드 블록만 출력하세요.**\n",
    "- 출력은 반드시 다음과 같은 **순수 JSON 형식의 코드 블록**으로 반환되어야 합니다:  \n",
    "\n",
    "Constraints(제약사항):  \n",
    "- **불필요한 필드는 출력하지 않습니다.**  \n",
    "- **각 항목별 정제 기준을 엄격하게 적용하여 오탐을 방지합니다.**  \n",
    "- 각 항목은 반드시 리스트([]) 형태로 출력해야 합니다.  \n",
    "- 해당 항목이 없으면 빈 배열([])로 출력합니다.  \n",
    "- 불필요한 설명, 분석 과정 없이 **JSON만 출력**해야 합니다.  \n",
    "- \"other\" 같은 불필요한 필드는 절대 포함하지 마세요.  \n",
    "- 출력은 반드시 JSON 형식으로 작성하세요.  \n",
    "- 답변은 한국어가 아닌 JSON 코드만 반환하세요.\n",
    "\n",
    "최종 출력은 반드시 아래 형식의 JSON 코드 블록만 포함되어야 합니다.  \n",
    "그 외의 텍스트, 분석, 해석은 절대 출력하지 마세요.\n",
    "\n",
    "예시:\n",
    "```json\n",
    "{\n",
    "  \"name\": [],\n",
    "  \"address\": [],\n",
    "  \"phone_number\": [\"],\n",
    "  \"email\": [],\n",
    "  \"id_number\": [],\n",
    "  \"resident_registration_number\": [],\n",
    "  \"bank_account\": [],\n",
    "  \"tracking_number\": []\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_text = r\"\"\"\n",
    "[심의요청] STUDIO TOMBOY\n",
    "고객명 : 김민지\n",
    "휴대폰 : 010-1111-2222\n",
    "구입일 : 2024-05-17\n",
    "구매금액 : 85,960원\n",
    "구매처 :(당사 주문은 주문 번호) SIV / 주문번호 202405173238829\n",
    "브랜드 : STUDIO TOMBOY\n",
    "구매점 : SIV\n",
    "SKU : 9104222391085OS\n",
    "상품명 : [2403632642] YOUTH 피그먼트 피케 티셔츠\n",
    "요청내용 :\n",
    "ㅁ 세탁 후 물 빠짐 / 전체적으로 발생, 흰색 줄 있는 듯한 모양으로 빠짐\n",
    "\"\"\"\n",
    "\n",
    "# 채팅 메시지 구성\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction.strip()},\n",
    "    {\"role\": \"user\", \"content\": input_text.strip()}\n",
    "]\n",
    "\n",
    "# 토큰화\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_ids = input_ids.to(device)\n",
    "model.to(device)\n",
    "\n",
    "streaming = True\n",
    "\n",
    "# 스트리밍 or 비스트리밍\n",
    "if streaming:\n",
    "    # 스트리머에 옵션 추가\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 생성 실행 스레드\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=2048,  # 넉넉하게 확보\n",
    "        do_sample=False,       # JSON 포맷 정확도 위해 비활성화 (선택)\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "\n",
    "    thread.join()  # 스트리밍이 끝날 때까지 기다림\n",
    "else:\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=2024,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    \n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"LGAI-EXAONE/EXAONE-Deep-7.8B-AWQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"E:/LLM/exaone-7.8b-awq\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"E:/LLM/exaone-7.8b-awq\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "#  프롬프트 구성\n",
    "instruction = r\"\"\"\n",
    "Role(역할지정):  \n",
    "개인정보 식별 및 JSON 변환 전문가로 활동하세요.\n",
    "\n",
    "Context(상황):  \n",
    "- 사용자가 입력한 문장에서 개인정보 항목을 식별하고, 이를 JSON 형식으로 변환해야 합니다.  \n",
    "- 문장에 포함되지 않은 정보는 null 값이 아닌 빈 배열 []로 출력해야 합니다.  \n",
    "- 각 항목별로 여러 개의 데이터가 있을 수 있으며, 이를 리스트(Array) 형태로 저장해야 합니다.  \n",
    "- 아래 명시된 JSON 구조 외의 다른 필드(예: \"other\")는 절대 포함하지 마세요.  \n",
    "- 각 항목은 정확한 기준에 따라 매칭해야 하며, 오탐을 방지해야 합니다.\n",
    "\n",
    "1. 이름(name)  \n",
    "- **여러 개의 이름이 존재하는 경우 리스트(Array) 형태로 반환해야 합니다.**  \n",
    "  - **\"고객명\", \"성함\", \"이름\"** 등의 키워드가 포함된 값은 반드시 \"name\" 필드로 저장해야 합니다.  \n",
    "  - **파일명, 제목, 메타데이터에 포함된 형식(예: '브랜드_이름', '작성자: 이름')도 탐지 대상에 포함**  \n",
    "  - **쉼표(,), ‘및’, ‘와/과’ 같은 연결어로 구분된 경우 각각 개별 요소로 추출** \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 직급(예: 대표, 부장, 차장, 과장, 사원 등)  \n",
    "  - 회사명(예: (주)위상, ABC Corp, Google Korea 등)  \n",
    "  - 존칭(예: 님, 씨, 선생님 등)  \n",
    "  - 서명 표현(예: 배상, 올림, 드림)  \n",
    "  - 쉼표(,) 이후 불필요한 정보(예: \"김철수, 마케팅팀\")  \n",
    "\n",
    "2. 주소(address)  \n",
    "- **여러 개의 주소가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 키워드가 포함된 문장을 주소로 인식:**  \n",
    "  - \"건물위치\", \"주소\", \"위치\", \"사업장 주소\", \"본사 주소\", \"지점 주소\"  \n",
    "- **다음 요소는 포함하지 마세요.**  \n",
    "  - 전화번호와 붙어 있는 경우 전화번호 제거  \n",
    "  - 주소 형식이 아닌 일반적인 지역 언급 제거 (예: \"상담 가능 지역: 서울, 경기\")  \n",
    "\n",
    "3. 전화번호(phone_number)  \n",
    "- **여러 개의 전화번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 국내 휴대전화번호 (예: 010-1234-5678)  \n",
    "  - 국내 일반전화 (예: 02-1234-5678)  \n",
    "  - 국제전화 형식 (예: +82-10-1234-5678)  \n",
    "- **숫자 조합이지만 전화번호가 아닌 경우 제거** (예: \"사업자 등록번호 123-45-67890\")  \n",
    "\n",
    "4. 이메일(email)  \n",
    "- **여러 개의 이메일이 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **이메일 형식을 만족하는 경우만 추출**  \n",
    "\n",
    "5. 신분증 번호(id_number)  \n",
    "- **여러 개의 신분증 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "- **다음 유형을 포함:**  \n",
    "  - 운전면허증 번호 (예: 12-34-567890-12)  \n",
    "  - 여권 번호 (예: M12345678)  \n",
    "\n",
    "6. 주민등록번호(resident_registration_number)  \n",
    "- **여러 개의 주민등록번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "7. 은행 계좌번호(bank_account)  \n",
    "- **여러 개의 은행 계좌번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "8. 운송장 번호(tracking_number)  \n",
    "- **여러 개의 운송장 번호가 포함될 수 있으므로 리스트(Array)로 반환해야 합니다.**  \n",
    "\n",
    "---\n",
    "\n",
    "Instructions(지시사항):  \n",
    "- 입력 문장에서 다음 8가지 항목을 식별하세요:  \n",
    "  1. name  \n",
    "  2. address  \n",
    "  3. phone_number  \n",
    "  4. email  \n",
    "  5. id_number  \n",
    "  6. resident_registration_number  \n",
    "  7. bank_account  \n",
    "  8. tracking_number  \n",
    "\n",
    "- 모든 항목은 반드시 리스트(Array) 형태로 출력해야 하며, 항목이 존재하지 않으면 빈 배열([])로 출력합니다.  \n",
    "- 절대로 \"other\" 필드나, 명시되지 않은 기타 필드는 포함하지 마세요.  \n",
    "- 분석, 해설, 주석 없이 **오직 JSON 코드 블록만 출력하세요.**\n",
    "- 출력은 반드시 다음과 같은 **순수 JSON 형식의 코드 블록**으로 반환되어야 합니다:  \n",
    "\n",
    "Constraints(제약사항):  \n",
    "- **불필요한 필드는 출력하지 않습니다.**  \n",
    "- **각 항목별 정제 기준을 엄격하게 적용하여 오탐을 방지합니다.**  \n",
    "- 각 항목은 반드시 리스트([]) 형태로 출력해야 합니다.  \n",
    "- 해당 항목이 없으면 빈 배열([])로 출력합니다.  \n",
    "- 불필요한 설명, 분석 과정 없이 **JSON만 출력**해야 합니다.  \n",
    "- \"other\" 같은 불필요한 필드는 절대 포함하지 마세요.  \n",
    "- 출력은 반드시 JSON 형식으로 작성하세요.  \n",
    "- 답변은 한국어가 아닌 JSON 코드만 반환하세요.\n",
    "\n",
    "최종 출력은 반드시 아래 형식의 JSON 코드 블록만 포함되어야 합니다.  \n",
    "그 외의 텍스트, 분석, 해석은 절대 출력하지 마세요.\n",
    "\n",
    "예시:\n",
    "```json\n",
    "{\n",
    "  \"name\": [],\n",
    "  \"address\": [],\n",
    "  \"phone_number\": [\"],\n",
    "  \"email\": [],\n",
    "  \"id_number\": [],\n",
    "  \"resident_registration_number\": [],\n",
    "  \"bank_account\": [],\n",
    "  \"tracking_number\": []\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_text = r\"\"\"\n",
    "[심의요청] STUDIO TOMBOY_최재식\n",
    "고객명 : 김민지\n",
    "휴대폰 : 010-1111-2222\n",
    "구입일 : 2024-05-17\n",
    "구매금액 : 85,960원\n",
    "구매처 :(당사 주문은 주문 번호) SIV / 주문번호 202405173238829\n",
    "브랜드 : STUDIO TOMBOY\n",
    "구매점 : SIV\n",
    "SKU : 9104222391085OS\n",
    "상품명 : [2403632642] YOUTH 피그먼트 피케 티셔츠\n",
    "요청내용 :\n",
    "ㅁ 세탁 후 물 빠짐 / 전체적으로 발생, 흰색 줄 있는 듯한 모양으로 빠짐\n",
    "\"\"\"\n",
    "\n",
    "# 채팅 메시지 구성\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction.strip()},\n",
    "    {\"role\": \"user\", \"content\": input_text.strip()}\n",
    "]\n",
    "\n",
    "# 토큰화\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_ids = input_ids.to(device)\n",
    "model.to(device)\n",
    "\n",
    "streaming = True\n",
    "\n",
    "# 정답 JSON\n",
    "ground_truth = {\n",
    "    \"name\": [\"최재식\", \"김민지\"],\n",
    "    \"address\": [],\n",
    "    \"phone_number\": [\"010-1111-2222\"],\n",
    "    \"email\": [],\n",
    "    \"id_number\": [],\n",
    "    \"resident_registration_number\": [],\n",
    "    \"bank_account\": [],\n",
    "    \"tracking_number\": []\n",
    "}\n",
    "\n",
    "# 성능 평가 함수\n",
    "def evaluate_generation(model, tokenizer, input_ids, ground_truth):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    input_ids = input_ids.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    gpu_mem_start = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    cpu_mem_start = psutil.virtual_memory().used\n",
    "    start_time = time.time()\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    gpu_mem_end = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    cpu_mem_end = psutil.virtual_memory().used\n",
    "\n",
    "    print(\"\\n📦 생성 결과:\")\n",
    "    print(output_text)\n",
    "\n",
    "    print(\"\\n⏱️ 속도/리소스 측정:\")\n",
    "    print(f\"- 처리 시간: {end_time - start_time:.2f}초\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"- GPU 메모리 사용: {(gpu_mem_end - gpu_mem_start) / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"- CPU 메모리 사용: {(cpu_mem_end - cpu_mem_start) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(output_text)\n",
    "        assert isinstance(parsed, dict), \"JSON 최상단은 객체여야 함\"\n",
    "        json_valid = True\n",
    "    except Exception as e:\n",
    "        json_valid = False\n",
    "        parsed = {key: [] for key in ground_truth}\n",
    "        print(f\"\\n❌ JSON 파싱 실패 또는 형식 오류: {e}\")\n",
    "\n",
    "    print(\"\\n📊 정확도 평가 (Precision / Recall / F1):\")\n",
    "    for key in ground_truth:\n",
    "        gt = set(ground_truth[key])\n",
    "        pred = set(parsed.get(key, [])) if json_valid else set()\n",
    "\n",
    "        tp = len(gt & pred)\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp else 1\n",
    "        recall = tp / (tp + fn) if tp + fn else 1\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "\n",
    "        print(f\"- {key}: P={precision:.2f} / R={recall:.2f} / F1={f1:.2f}\")\n",
    "\n",
    "    return parsed\n",
    "\n",
    "# 평가 함수 실행\n",
    "evaluate_generation(model, tokenizer, input_ids, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
